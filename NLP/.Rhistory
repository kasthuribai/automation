getwd()
getwd()
2+3
install.packages("dplyr")
a =5
type(a)
typeof(a)
number <- readline(prompt = "")
number1= number/2
cat("Divided by 2 gives:\n");str(number1)
number <- readline(prompt = "")
myfunt <- function(){
cat("Enter an integer or whole number : \n")
enter <- readline(prompt = "")
cat("You sumitted : \n"); str(enter)
}
number <- as.integer(readline(prompt = ""))
number <- as.integer(readline(prompt = ""))
number1= number/2
cat("Divided by 2 gives:\n");str(number1)
number <- as.integer(readline(prompt = ""))
cat("The number you entered is");str(number)
number1= number/2
cat("Divided by 2 gives:\n");str(number1)
?print
a <- 5
if a>2,print("a is greater",a)
if a>2print("a is greater",a)
if (a>2, print(a))
if
sfdf
if(a>5){}
if(a>5){print("yes")}
if(a>5){print("yes")}elsif{print("no")}
if(a>5){print("yes")}ifelse{print("no")}
if(a>5){print("yes")}else{print("no")}
age <-20
switch(age,
"18"= {print("Adult")}),
"10"= {print("child")})
age <-20
switch(age,
"18"= {print("Adult")},
"10"= {print("child")})
age <-20
switch(age,
"18"= {print("Adult")},
"10"= {print("child")})
age <-"18"
switch(age,
"18"= {print("Adult")},
"10"= {print("child")})
age <-"19"
switch(age,
"18"= {print("Adult")},
"10"= {print("child")})
vec <- c(2,3,4,5)
print vec
vec <- c(2,3,4,5)
print (vec)
vec <- c(2,3,4,5)
print (vec)
for val in vec
{
print(val+2)
}
vec <- c(2,3,4,5)
print (vec)
library(dplyr)
for val in vec
{
val++
print(val+2)
}
vec <- c(2,3,4,5)
print (vec)
library(dplyr)
for val in vec
{
val=val+2
print(val)
}
vec <- c(2,3,4,5)
print (vec)
library(dplyr)
val <- 0
for val in vec
{
val=val+2
print(val)
}
library(dplyr)
vec <- c(2,3,4,5)
print (vec)
for val in vec
{
val=val+2
print(val)
}
library(dplyr)
vec <- c(2,3,4,5)
print (vec)
for (val in vec)
{
val=val+2
print(val)
}
sales = 10
price = 10
Net = sales * price
sales = 10
price = 10
Net = sales * price
Net
data()
mtcars
nnrow(mtcars)
nrow(mtcars)
dim(mtcars)
install.packages("healthcareai")
library(healthcareai)
View(healthcareai)
nrows(heathcareai)
library(healthcareai)
which r
install.packages("installr") # install installr
library(installr)
library(dplyr)
mtcars
View(mtcars)
library(dplyr)
mtcars <- mtcars
counts <- table(mtcars$gear)
counts
barplot(counts)
# horizontal barchart
barplot(counts, horiz =TRUE)
# Load
library(tidytext)
library(dplyr)
library(stringr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
tfile = readLines('covid.txt', encoding="UTF-8")
setwd("~/GitHub/NLP")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile = readLines('covid.txt', encoding="UTF-8")
tfile
docs <- Corpus(VectorSource(tfile))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Sentimental Analysis
get_sentiments("bing")
# Sentimental Analysis
get_sentiments("bing")
get_sentiments("afinn")
get_sentiments("nrc")
unique(get_sentiments("nrc")$sentiment)
unique(get_sentiments("bing")$sentiment)
tokens <- data_frame(text = tfile) %>% unnest_tokens(word, text)
tokens
# Using bing lexicon to categorize to postive and negative
# and to get net positive minus negative score
tokens %>%
inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
count(sentiment) # count the # of positive & negative words
# Using nrc lexicon to categorize to multiple sentiments
tokens %>%
inner_join(get_sentiments("nrc")) %>% # pull out only sentiment words
count(sentiment)
